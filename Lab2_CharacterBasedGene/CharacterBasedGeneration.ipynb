{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJRhl22NdNFA"
   },
   "source": [
    "## Training a character language model and studying various ways of generating text\n",
    "\n",
    "**Author: matthieu.labeau@telecom-paris.fr**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- We will train a network to predict a next character given an input sequence of characters, and use it to generate new sequences.\n",
    "- We will strictly work with local (and not structured, meaning we will only one character at a time) prediction - however, we will look into a relatively simple heuristic to improve the \"structure\": *beam search*. We can also try to improve generation with other methods: *temperature* sampling, *top-k* sampling, *top-p* sampling,\n",
    "- We will use ```keras```to build the model based on a **recurrent neural network** called a **LSTM**, which will use simple features (one-hot vector representing previous characters) to predict the next characters. We will use a small model to avoid training for too long. *Remark: you don't need to know how this model works - just its inputs and outputs !*\n",
    "- We will use a small dataset (poetry, from project Gutenberg) - you can use any data you prefer, as long as you are able to train the model on it.\n",
    "- Even with a small dataset and a small model, training may be long. If you can use a computing infrastructure, like Google colab, it may be more practical - and you probably can obtain better results by using a bigger model and a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JfjG_pve1Ds"
   },
   "source": [
    "#### Obtaining the data\n",
    "- We download directly the ebook from project Gutenberg - you can get any other text you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.17.3-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (81 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./.local/lib/python3.10/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.32.5)\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.14.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.75.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.5 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./.local/lib/python3.10/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting protobuf>=5.28.0\n",
      "  Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Requirement already satisfied: keras>=3.10.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: optree in ./.local/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: rich in ./.local/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in ./.local/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, termcolor, tensorboard-data-server, protobuf, pillow, opt_einsum, MarkupSafe, markdown, grpcio, google_pasta, gast, astunparse, werkzeug, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.0 libclang-18.1.1 markdown-3.9 opt_einsum-3.4.0 pillow-11.3.0 protobuf-6.32.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BCd_FKg1chKU"
   },
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "url = 'http://www.gutenberg.org/cache/epub/6099/pg6099.txt'\n",
    "path = get_file('pg6099.txt', origin=url)\n",
    "\n",
    "f = open(path, 'r' , encoding = 'utf8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "\n",
    "start = False\n",
    "for line in lines:\n",
    "    if(\"*** START OF THE PROJECT GUTENBERG EBOOK LES FLEURS DU MAL ***\" in line and start==False):\n",
    "        start = True\n",
    "    if(\"            *** END OF THE PROJECT GUTENBERG EBOOK LES FLEURS DU MAL ***\" in line):\n",
    "        break\n",
    "    if(start==False or len(line) == 0):\n",
    "        continue\n",
    "    text.append(line)\n",
    "\n",
    "f.close()\n",
    "text = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrHl4Y3SEeWB",
    "outputId": "3a55fdd2-740e-4d17-d9b6-237a00a5fe1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ces plaintes,\n",
      "   Ces extases, ces cris, ces pleurs, ces _Te Deum,_\n",
      "   Sont un écho redit par mille labyrinthes;\n",
      "   C'est pour les coeurs mortels un divin opium.\n",
      " \n",
      "   C'est un cri répété par mille sentinelles,\n",
      "   Un ordre renvoyé par mille porte-voix;\n",
      "   C'est un phare allumé sur mille citadelles,\n",
      "   Un appel de chasseurs perdus dans les grands bois!\n",
      " \n",
      "   Car c'est vraiment, Seigneur, le meilleur témoignage\n",
      "   Que nous puissions donner de notre dignité\n",
      "   Que cet ardent sanglot qui roule d'âge en âge\n",
      "   Et vient mourir au bord de votre éternité!\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   LA MUSE VENALE\n",
      " \n",
      " \n",
      "   O Muse de mon coeur, amante des palais,\n",
      "   Auras-tu, quand Janvier lâchera ses Borées,\n",
      "   Durant les noirs ennuis des neigeuses soirées,\n",
      "   Un tison pour chauffer tes deux pieds violets?\n",
      " \n",
      "   Ranimeras-tu donc tes épaules marbrées\n",
      "   Aux nocturnes rayons qui percent les volets?\n",
      "   Sentant ta bourse à sec autant que ton palais,\n",
      "   Récolteras-tu l'or des voûtes azurées?\n",
      " \n",
      "   Il te faut, pour gagner ton pain de chaq\n"
     ]
    }
   ],
   "source": [
    "print(text[20000:21000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN-Cq6PGe61b"
   },
   "source": [
    "#### Keeping track of possible characters\n",
    "- Using a ```set```, create a sorted list of possible characters\n",
    "- Create two dictionnaries, having characters and corresponding indexes as {key: value}, and reverse.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "chars = [a, b, c]\n",
    "```\n",
    "\n",
    "```python\n",
    "chars_indices = {a: 0, b: 1, c: 2}\n",
    "```\n",
    "\n",
    "```python\n",
    "indices_chars = {0: a, 1: b, 2: c}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdbvrYqHe9cg",
    "outputId": "5c08efad-34be-40f0-8bf8-0557d0f5877e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 179332\n",
      "Unique char list:['\\n', '-', '7', 'E', 'O', 'Y', 'g', 'q', '«', 'ê', '’']\n",
      "Total number of characters: 105\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '%': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, '[': 51, ']': 52, '_': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, '«': 80, '»': 81, 'È': 82, 'É': 83, 'Ï': 84, 'à': 85, 'â': 86, 'ç': 87, 'è': 88, 'é': 89, 'ê': 90, 'ë': 91, 'î': 92, 'ï': 93, 'ô': 94, 'ù': 95, 'û': 96, 'ü': 97, '—': 98, '‘': 99, '’': 100, '“': 101, '”': 102, '•': 103, '™': 104}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '?', 26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'E', 31: 'F', 32: 'G', 33: 'H', 34: 'I', 35: 'J', 36: 'K', 37: 'L', 38: 'M', 39: 'N', 40: 'O', 41: 'P', 42: 'Q', 43: 'R', 44: 'S', 45: 'T', 46: 'U', 47: 'V', 48: 'W', 49: 'X', 50: 'Y', 51: '[', 52: ']', 53: '_', 54: 'a', 55: 'b', 56: 'c', 57: 'd', 58: 'e', 59: 'f', 60: 'g', 61: 'h', 62: 'i', 63: 'j', 64: 'k', 65: 'l', 66: 'm', 67: 'n', 68: 'o', 69: 'p', 70: 'q', 71: 'r', 72: 's', 73: 't', 74: 'u', 75: 'v', 76: 'w', 77: 'x', 78: 'y', 79: 'z', 80: '«', 81: '»', 82: 'È', 83: 'É', 84: 'Ï', 85: 'à', 86: 'â', 87: 'ç', 88: 'è', 89: 'é', 90: 'ê', 91: 'ë', 92: 'î', 93: 'ï', 94: 'ô', 95: 'ù', 96: 'û', 97: 'ü', 98: '—', 99: '‘', 100: '’', 101: '“', 102: '”', 103: '•', 104: '™'}\n"
     ]
    }
   ],
   "source": [
    "print('Corpus length:', len(text))  \n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(f\"Unique char list:{chars[::10]}\")\n",
    "print('Total number of characters:', len(chars))\n",
    "char_indices = {char: idx for idx, char in enumerate(chars)}\n",
    "indices_char = {idx: char for idx, char in enumerate(chars)}\n",
    "print(char_indices)\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzqwN_5gfEcQ"
   },
   "source": [
    "#### Creating training data\n",
    "- We will represent characters using *one-hot vectors*. Hence, the i-th character of n possible characters will be represented by a vector of length $n$, containing $0$ expect for a $1$ in position $i$. Following our previous examples, ```a = [1, 0, 0]``` and ```b = [0, 1, 0]```.\n",
    "- Hence, a sequence of characters is a list of one-hot vectors. Our goal will be to predict, given an input sequence of fixed length (here, this length is given by ```maximum_seq_length```) the next character. Hence, we need to build two lists: ```sentences```, containing the input sequences, and ```next_char``` the characters to be predicted.\n",
    "- We do not necessarily need to take all possible sequences. We can select one every ```time_step``` steps.\n",
    "\n",
    "Example: Using the previous dictionnaries, the sequence:\n",
    "```'acabbaccaabba'``` with ```maximum_seq_length = 4``` and ```time_step = 2``` would give the following lists:\n",
    "\n",
    "```python\n",
    "sentences = ['acab', 'abba', 'bacc', 'ccaa', 'aabb']\n",
    "```\n",
    "\n",
    "```python\n",
    "next_char = ['b', 'c', 'a', 'b', 'a']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIIvHDZvfHn7",
    "outputId": "dbf65c75-d398-49a9-a146-e51b4263b0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sequences: 179308\n"
     ]
    }
   ],
   "source": [
    "maximum_seq_length = 24\n",
    "time_step = 1\n",
    "sentences = []\n",
    "next_char = []\n",
    "for i in range(0, len(text)- maximum_seq_length, time_step):\n",
    "    sentences.append(text[i:i+maximum_seq_length])\n",
    "    next_char.append(text[i+maximum_seq_length])\n",
    "\n",
    "print('Number of Sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "UYa-53Qqfcac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7z7a6_7femM"
   },
   "source": [
    "#### Creating training tensors\n",
    "- We need to transform these lists into tensors, using one-hot vectors to represent characters.\n",
    "- We will need 3 dimensions for the training examples from ```sentences```: the number of examples, the length of the sequence, and the dimension of the one-hot vector\n",
    "- This is reduced to 2 dimensions for the ```next_char```: number of examples and one-hot vector.\n",
    "\n",
    "Example: the previous ```sentences``` would become:\n",
    "\n",
    "```python\n",
    "X = [[[1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[0, 1, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [0, 0, 1]],\n",
    "     [[0, 0, 1],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0]]]\n",
    "```\n",
    "       \n",
    "```python\n",
    "y = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "oPQEWEKnfgAD"
   },
   "outputs": [],
   "source": [
    "num_chars = len(chars)\n",
    "X = np.zeros((len(sentences), maximum_seq_length, num_chars), dtype=np.bool_)\n",
    "y = np.zeros((len(sentences), num_chars), dtype=np.bool_)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_char[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC6jUjYYjPHj"
   },
   "source": [
    "#### Implement the model\n",
    "In order to implement the model as simply as possible, we will use ```keras```. It allows to create models with only a few lines of code.\n",
    "First, we will create a very simple model based on a **LSTM**, which is a *recurrent* architecture. Note that one the strength of a recurrent architecture is to allow for inputs of varying length - here, to simplify data processing, we will keep a **fixed input size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "gGIn8bKEfgGZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s_ywdkTNRtA"
   },
   "source": [
    "We need to create a LSTM model that takes directly out inputs from ```X``` and try to predict one-hot vectors from ```y```.\n",
    "- What are the input and output dimensions ?\n",
    "  - ```X```: size of the dataset $\\times$ maximum sequence length $\\times$ vocabulary size\n",
    "  - ```y```: size of the dataset $\\times$ vocabulary size\n",
    "- The model should be made with a ```LSTM``` layer, and a ```Dense``` layer followed by a softmax activation function. Work out the intermediate dimensions:\n",
    "  - ```X``` $\\rightarrow$ (LSTM) $\\rightarrow$ ```h``` $\\rightarrow$ (Dense) $\\rightarrow$ ```s``` $\\rightarrow$ (softmax) $\\rightarrow$ ```pred```\n",
    "  - Look at layers arguments and find out to proper ```input_shape``` for the ```LSTM``` layer and the proper size for the ```Dense``` layer.\n",
    "  - We can use 256 as the size of hidden states for the ```LSTM```.\n",
    "- We will minimize ```cross-entropy(pred, y)```. Use the ```categorical_crossentropy``` loss, with the optimizer of your preference (for example, ```adam```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gzIRMSx1gJRh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 11:00:35.763913: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/ambroise012/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maximum_seq_length, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slRjcDrkNZ7I"
   },
   "source": [
    "We will now only need a few functions to use this model:\n",
    "- ```model.fit```, which you will call on the appropriately processed data ```X, y```\n",
    "- ```model.predict```, which we will use on an input **of the same dimension of X** to output the probabilities. That includes the *first one*, corresponding to the number of examples in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XguFAVGjnVG"
   },
   "source": [
    "#### Create a function to generate text with our model\n",
    "- We use the output of our model to select the next most probable character (with the ```argmax``` function)\n",
    "- We need to transform an input text into an input tensor, as before (taking the right length, the last ```maximum_seq_length``` characters)\n",
    "- We need to transform back the most probable index into a character and add it to our text.\n",
    "- This must be looped ```num_generated``` times, each time obtaining a new input tensor from the new input sequence (which has the character we previously predicted at the end !)\n",
    "\n",
    "\n",
    "We can begin by writing a function facilitating the transfer between text and tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LIS3MhQOcSX"
   },
   "outputs": [],
   "source": [
    "def get_tensor(sentence, voc):\n",
    "    x = np.zeros((1, len(sentence), len(voc)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[0, t, voc[char]] = 1.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBhnsrCWOc-q"
   },
   "source": [
    "The following function (```end_epoch_generate```) is here to facilitate automatic generation at the end of each epoch, so you can monitor of generation changes as the model trains. It calls the ```generate_next``` function upon each sequence of text in ```texts_ex```. The only element in this list right now comes from the training data - you can add your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "C-U3odSfkS70"
   },
   "outputs": [],
   "source": [
    "def generate_next(model, text, num_generated=120):\n",
    "    \"\"\" \n",
    "    Generates new text, character by character, using a trained model. \n",
    "    It starts from an initial input text and predicts the next character repeatedly for a specified number of steps.\n",
    "    \"\"\"\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = get_tensor(sentence, char_indices)\n",
    "        predictions = model.predict(x,verbose=0)[0]\n",
    "        next_index = np.argmax(predictions)\n",
    "        next_char = indices_char[next_index] \n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)\n",
    "\n",
    "def end_epoch_generate(epoch, _):\n",
    "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
    "    texts_ex = [\"La sottise, l'erreur, le péché\"]\n",
    "    for text in texts_ex:\n",
    "        sample = generate_next(model, text)\n",
    "        print('%s' % (sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "yC7yZ4LZkTBj",
    "outputId": "15ef6ffd-4dde-430e-c209-57e44e3d7261"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"La sottise, l'erreur, le péchéù•0ùtB0Bkqk[t[.K?»çQn‘Éùù   ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù  ù \""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = \"La sottise, l'erreur, le péché\"\n",
    "generate_next(model, text_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7hDj4zvkTEv",
    "outputId": "86a8c29f-5231-4814-fb3e-84c5f18316c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 2.8860\n",
      " Generating text after epoch: 1\n",
      "La sottise, l'erreur, le péchése de coure de coure de coure de coure de coure de coure de coure de coure de coure de coure de coure de coure de coure \n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 82ms/step - loss: 2.5372 - val_loss: 2.7942\n",
      "Epoch 2/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 2.1032\n",
      " Generating text after epoch: 2\n",
      "La sottise, l'erreur, le péchéses de soures et le soures et le soures et le soures et le soures et le soures et le soures et le soures et le soures et\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 78ms/step - loss: 2.0646 - val_loss: 2.7545\n",
      "Epoch 3/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.9656\n",
      " Generating text after epoch: 3\n",
      "La sottise, l'erreur, le péchés des les ples les ples les ples les ples les ples les ples les ples les ples les ples les ples les ples les ples les pl\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 77ms/step - loss: 1.9415 - val_loss: 2.6912\n",
      "Epoch 4/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.8698\n",
      " Generating text after epoch: 4\n",
      "La sottise, l'erreur, le péchétes de comme un comme un comme un comme un comme un comme un comme un comme un comme un comme un comme un comme un comme\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 78ms/step - loss: 1.8548 - val_loss: 2.6585\n",
      "Epoch 5/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.7928\n",
      " Generating text after epoch: 5\n",
      "La sottise, l'erreur, le péchéres et les soures des mortes des mortes des mortes des mortes des mortes des mortes des mortes des mortes des mortes des\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 78ms/step - loss: 1.7850 - val_loss: 2.6660\n",
      "Epoch 6/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.7287\n",
      " Generating text after epoch: 6\n",
      "La sottise, l'erreur, le péchés de les paraits de la pour de les partises de la pour de les contes de la pour de la pour de les contes de la pour de l\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 78ms/step - loss: 1.7393 - val_loss: 2.6706\n",
      "Epoch 7/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.7047\n",
      " Generating text after epoch: 7\n",
      "La sottise, l'erreur, le péché de sour des charmes des charsses des charsses des charsses des charsses des charsses des charsses des charsses des char\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 79ms/step - loss: 1.6892 - val_loss: 2.6718\n",
      "Epoch 8/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.6425\n",
      " Generating text after epoch: 8\n",
      "La sottise, l'erreur, le péché de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 85ms/step - loss: 1.6434 - val_loss: 2.6448\n",
      "Epoch 9/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.6122\n",
      " Generating text after epoch: 9\n",
      "La sottise, l'erreur, le péchétit de la morte et le plus trouver les partistes de la pour de la morte et le soleil de la morte de son enfant de la mor\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 84ms/step - loss: 1.6067 - val_loss: 2.6628\n",
      "Epoch 10/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 1.5727\n",
      " Generating text after epoch: 10\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 90ms/step - loss: 1.5728 - val_loss: 2.6857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f46f87adb10>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72FpUqWun0WI"
   },
   "source": [
    "#### Using character embeddings\n",
    "- Instead of using one-hot vectors to represent characters, we will now use character embeddings, which are vectors belonging to the same space.\n",
    "- We will need as many vectors as there is characters. The input of the network will be simpler, since we will just need to indicate to the model which character is in input.\n",
    "- The output does not change: indeed, Keras uses one-hot vectors for the target of the categorical cross-entropy loss.\n",
    "Example: the previous example ```sentences``` would now become:\n",
    "- We need to add a ```Embedding``` layer to the model, with the right input size, and to choose which dimension use for our embeddings.\n",
    "\n",
    "```python\n",
    "X = [[0, 2, 0, 1],\n",
    "     [0, 1, 1, 0],\n",
    "     [1, 0, 2, 2],\n",
    "     [2, 2, 0, 0],\n",
    "     [0, 0, 1, 1]]\n",
    "```\n",
    "       \n",
    "```python\n",
    "y = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "myi-eu8cuJxF"
   },
   "outputs": [],
   "source": [
    "X_emb = np.zeros((len(sentences), maximum_seq_length), dtype=int)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_emb[i,t] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "_2QkLn7itmIF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ambroise012/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model_emb = Sequential()\n",
    "model_emb.add(Embedding(len(chars), 32, input_length = maximum_seq_length))\n",
    "model_emb.add(LSTM(256))\n",
    "model_emb.add(Dense(len(chars)))\n",
    "model_emb.add(Activation('softmax'))\n",
    "\n",
    "model_emb.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sK70WaUoltY",
    "outputId": "f9e0eb5f-8d09-4931-b5fb-1b9363d518f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 2.7910\n",
      " Generating text after epoch: 1\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 88ms/step - loss: 2.4251 - val_loss: 2.7580\n",
      "Epoch 2/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 2.0230\n",
      " Generating text after epoch: 2\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 82ms/step - loss: 1.9798 - val_loss: 2.6977\n",
      "Epoch 3/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.8653\n",
      " Generating text after epoch: 3\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 82ms/step - loss: 1.8393 - val_loss: 2.6555\n",
      "Epoch 4/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.7552\n",
      " Generating text after epoch: 4\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 79ms/step - loss: 1.7406 - val_loss: 2.6466\n",
      "Epoch 5/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.6715\n",
      " Generating text after epoch: 5\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 81ms/step - loss: 1.6661 - val_loss: 2.6548\n",
      "Epoch 6/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.6103\n",
      " Generating text after epoch: 6\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 79ms/step - loss: 1.6039 - val_loss: 2.6433\n",
      "Epoch 7/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.5579\n",
      " Generating text after epoch: 7\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 82ms/step - loss: 1.5519 - val_loss: 2.6532\n",
      "Epoch 8/10\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.5039\n",
      " Generating text after epoch: 8\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 84ms/step - loss: 1.5058 - val_loss: 2.6851\n",
      "Epoch 9/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.4587\n",
      " Generating text after epoch: 9\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 83ms/step - loss: 1.4633 - val_loss: 2.7367\n",
      "Epoch 10/10\n",
      "\u001b[1m1120/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.4226\n",
      " Generating text after epoch: 10\n",
      "La sottise, l'erreur, le péché de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la pare de la par\n",
      "\u001b[1m1121/1121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 81ms/step - loss: 1.4246 - val_loss: 2.7212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f46f873d150>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb.fit(X_emb, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ATJnocork5a"
   },
   "source": [
    "#### Sampling with our model\n",
    "- Now, instead of simply selecting the most probable next character, we would like to be able to draw a sample from the distribution output by the model.\n",
    "- To better control the generation, we would like to use the argument ```temperature```, to smooth the distribution.\n",
    "- We will use the ```multinomial``` function from the ```random``` package to draw samples.\n",
    "- We integrate this into a function ```generate_sample``` that is almost exactly like ```generate_next```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔹 1. Température appliquée à une distribution\n",
    "\n",
    "Ton modèle sort une distribution de probabilités $p_i$ (ex. via `softmax`).\n",
    "On veut la transformer avec la **température $T$** :\n",
    "\n",
    "$$\n",
    "q_i = \\frac{\\exp\\left(\\frac{\\log(p_i)}{T}\\right)}{\\sum_j \\exp\\left(\\frac{\\log(p_j)}{T}\\right)}\n",
    "$$\n",
    "\n",
    "👉 en pratique c’est équivalent à :\n",
    "\n",
    "$$\n",
    "q_i = \\frac{p_i^{1/T}}{\\sum_j p_j^{1/T}}\n",
    "$$\n",
    "\n",
    "* Si $T=1$, alors $q_i = p_i$ (pas de changement).\n",
    "* Si $T < 1$, les grandes probabilités deviennent encore plus grandes → choix plus déterministe.\n",
    "* Si $T > 1$, les probabilités s’aplatissent → plus de hasard.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Tirage multinomial (`sample`)\n",
    "\n",
    "Une fois que tu as $q_i$, tu tires **un index** aléatoirement selon cette distribution :\n",
    "\n",
    "$$\n",
    "\\text{next\\_index} \\sim \\text{Multinomial}(1, q)\n",
    "$$\n",
    "\n",
    "C’est-à-dire :\n",
    "\n",
    "* Tu tires un seul échantillon.\n",
    "* La probabilité de tirer l’index $i$ est exactement $q_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Génération de texte\n",
    "\n",
    "Le processus itératif est :\n",
    "\n",
    "$$\n",
    "\\text{seq}_{t+1} = \\text{seq}_t \\;+\\; \\text{next\\_char}\n",
    "$$\n",
    "\n",
    "avec\n",
    "\n",
    "$$\n",
    "\\text{next\\_char} = \\text{argmax}(\\text{sample}(q))\n",
    "$$\n",
    "\n",
    "ou plus simplement \"le caractère choisi aléatoirement selon la distribution tempérée\".\n",
    "\n",
    "---\n",
    "\n",
    "✅ Donc dans ton code :\n",
    "\n",
    "* **`reweight(predictions, temperature)`** applique la formule\n",
    "\n",
    "  $$\n",
    "  q_i = \\frac{p_i^{1/T}}{\\sum_j p_j^{1/T}}\n",
    "  $$\n",
    "\n",
    "* **`sample(predictions, temperature)`** tire un indice selon $q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "psIfwKiLol3W"
   },
   "outputs": [],
   "source": [
    "def reweight(predictions, temperature):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    return predictions\n",
    "\n",
    "def sample(predictions, temperature):\n",
    "    predictions = reweight(predictions, temperature)\n",
    "    sampled = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(sampled)\n",
    "\n",
    "def generate_sample(model, text, num_generated=120, temperature=1.0):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(predictions, temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy5z2vW1kTHy",
    "outputId": "f8393f84-ff69-4ed5-dae8-c15ec55c17c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péchés que dont de sourir;\n",
      " \n",
      "   La Morts de la Plaisir comme un soleil opard, charché ma forte,\n",
      "   Sous dont le sous le nouve\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(model_emb, text_ex, temperature = 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "U_wD4v8Qy-nG"
   },
   "outputs": [],
   "source": [
    "def sample_top_k(predictions, temperature, k):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    indices_to_remove = log_predictions.argsort()[:k]\n",
    "    log_predictions[indices_to_remove] = -float('Inf')\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_sample_top_k(model, text, num_generated=120, temperature=1.0, k=10):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample_top_k(predictions, temperature, k)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hXKdtloy-rm",
    "outputId": "e3cf1ebe-389f-49e1-cb03-af1a08bb9c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péchés de son oeil ficle bier qu'un sombre comme un gâte avec la longue\n",
      "   Appernelle plus qu'in élernaire infeins le mardit \n"
     ]
    }
   ],
   "source": [
    "print(generate_sample_top_k(model_emb, text_ex, temperature = 0.8, k = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa1z4yJzrvMp"
   },
   "source": [
    "#### Generate text with the beam algorithm\n",
    "- We need to loop for each character we want to generate, keeping track of the best ```beam_size``` sequences at the most.\n",
    "- Besides keeping track of past generated character for each of these ```beam_size``` sequences, we need to keep track of their log-probability.\n",
    "- This is done by, at each loop, keeping the ```beam_size```best predictions for each of the ```beam_size``` sequences, computing the log-probabilities of the newly formed (```beam_size```)$^2$ , and keeping the overall ```beam_size``` best new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "5FTEE4Aqz55B"
   },
   "outputs": [],
   "source": [
    "def generate_beam(model, text, beam_size=16, num_generated=128):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    # Initialization of the beam with log-probabilities for the sequence\n",
    "    current_beam = [(0, [], sentence)]\n",
    "\n",
    "    for l in range(num_generated):\n",
    "        all_beams = []\n",
    "        for prob, current_preds, current_input in current_beam:\n",
    "            x = np.zeros((1, maximum_seq_length))\n",
    "            for t, char in enumerate(current_input):\n",
    "                x[0, t] = char_indices[char]\n",
    "            prediction = model.predict(x = x, verbose = 0)[0]\n",
    "            possible_next_chars = prediction.argsort()[-beam_size:][::-1]\n",
    "            all_beams += [\n",
    "                (prob + np.log(prediction[next_index]),\n",
    "                 current_preds + [next_index],\n",
    "                 current_input[1:] + indices_char[next_index]\n",
    "                )\n",
    "                for next_index in possible_next_chars]\n",
    "\n",
    "        current_beam = sorted(all_beams)[-beam_size:]\n",
    "\n",
    "    return text + ''.join([indices_char[idx] for idx in current_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rk0YnN8mz6Cs",
    "outputId": "72069614-52a8-421a-efa0-ad4832ad2d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péché dans les souvenirs.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   LE CHARE\n",
      " \n",
      " \n",
      "   Je suis le soleil comme un soleil comme un grand comme un soleil comme un grand\n"
     ]
    }
   ],
   "source": [
    "print(generate_beam(model_emb, text_ex))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
