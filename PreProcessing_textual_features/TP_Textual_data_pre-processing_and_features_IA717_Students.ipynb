{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c13ac7c",
   "metadata": {},
   "source": [
    "# Lab : Textual data, pre-processing and features\n",
    "\n",
    "**Author: matthieu.labeau@telecom-paris.fr**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Confront yourself to the difficulties of retrieving and using (annotated) datasets.\n",
    "2. Learn to clean and process textual data. Try several methods to represent this data.\n",
    "3. Visualize these representations, and reflect on their potential usefulness for the dedicated task. \n",
    "\n",
    "\n",
    "## Necessary dependancies\n",
    "\n",
    "We require the usual python package for storing and processing data:\n",
    "- ```numpy```,  \n",
    "- ```pandas```,\n",
    "- ```altair``` (for visualisation)\n",
    "- ```matplotlib``` (for pre-processing)\n",
    "\n",
    "We will take a quick look at *crawling* for data:\n",
    "- ```beautifulsoup```,\n",
    "\n",
    "Besides, we will need the following packages for data pre-processing, representation and visualization:\n",
    "- The Machine Learning API Scikit-learn ```sklearn``` : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit ```nltk```: http://www.nltk.org/install.html\n",
    "- ```gensim```: https://radimrehurek.com/gensim/\n",
    "\n",
    "And to get a taste of modern models:\n",
    "- ```transformers```\n",
    "- And you will need to install other small libraries to make the model run: ```sentencepiece```\n",
    "\n",
    "**You can install all of these libraries via ```pip``` !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902f5a4",
   "metadata": {},
   "source": [
    "## I - Retrieving annotated datasets\n",
    "\n",
    "Public annotated datasets are usually designed with a specific **task** in mind and published by researchers, along with a dedicated research paper, which explicits how it was built, its caracteristics, and proposes some baseline solutions to solve the task.\n",
    "\n",
    "We will look at the dataset given in [this paper](https://aclanthology.org/2024.lrec-main.73/). It seems interesting: it contains fine-grained annotations made by experts, and compare them with automated models designed to detect fake news. The data is available in [their github](https://github.com/obs-info/obsinfox). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7416b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"NLP_labs/PreProcessing_textual_features/obsinfox.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ba67",
   "metadata": {},
   "source": [
    "Let's look at the categories shown. A lot of labels are given, and there is several annotators for each articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b9688",
   "metadata": {},
   "source": [
    "Understand the structure of the table. Find a solution to get a list of all URLs and titles, without duplicates: \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url_title = news[['URL', 'Title']]\n",
    "\n",
    "# remove duplicate\n",
    "clean_df_url_title = df_url_title.drop_duplicates()\n",
    "print(clean_df_url_title.shape)\n",
    "print(clean_df_url_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6e55b",
   "metadata": {},
   "source": [
    "You should see that you have $100$ examples - while we have the titles, it will clearly not be enough to find out which label applies. In order to get the text of the articles, we need to use the provided URLs.\n",
    "This is actually something that is quite frequent, usually for certain datasets with copyright or licensing issues, in particular when third parties are forbidden to distribute the textual data themselves. \n",
    "\n",
    "\n",
    "As you can expect, this might cause issues in the long run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c09d4",
   "metadata": {},
   "source": [
    "How to get the text ? Well, we can use a crawler which will take a look at the urls, get the html, and find the text associated to the article thanks to the html parser.\n",
    "Happily, there exists a html tag specifically for this: ```article```. \n",
    "If you would like to know more, take a look at the [beautiful soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text). We can propose code which roughly does what we want: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gives as the page as a BeautifulSoup object\n",
    "# It represents the document as a nested data structure\n",
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# This function will look for the text under the tag \"article\"\n",
    "# If there is not, it will get whatever text which is not labelled as 'style' or 'script' and remove empty lines\n",
    "def get_article(url):\n",
    "    item = get_soup(url)\n",
    "    if (item.article == None):\n",
    "        for script in item(['style', 'script']):\n",
    "            script.extract()    \n",
    "        text = item.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        text = '\\n'.join(line for line in lines if line)\n",
    "        return text\n",
    "    else:\n",
    "        return item.article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a47341",
   "metadata": {},
   "source": [
    "Apply this to create a new column ```Text``` to our previously created table. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc91151",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_url_title['Text'] = clean_df_url_title[\"URL\"].apply(lambda x: get_article(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb2b20",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Look at the results. What are the different issues encountered ? How could we try to solve them ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952592a",
   "metadata": {},
   "source": [
    "- some URL are not accessible anymore\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b5338",
   "metadata": {},
   "source": [
    "This are issues you might encounter, but **do not solve them now ! You can directly move on to the next part.** Next, we will turn to easier-to-access data to work on cleaning and pre-processing text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c37d0",
   "metadata": {},
   "source": [
    "## II - Symbolic document representations\n",
    "\n",
    "Let's give up on this, and find some data that is directly available... you should note that with a lot of user-generated content coming from websites like twitter and reddit, who have made their policies more severe over the year (data only to be accessed through their API; access then became paying) !\n",
    "\n",
    "Still, we can use this [paper](https://aclanthology.org/2020.lrec-1.175/), which propose tweets annotated for the task of *sexism detection*. The data can be found in their [github](https://github.com/aollagnier/Sexism_Twitter_French/tree/main). Take a look at the description of the categories. \n",
    "\n",
    "We will:\n",
    "1. Clean and pre-process this data as needed.\n",
    "2. Obtain **symbolic** representations for each tweet, and look at their similarities. We will look at several ways of doing so.\n",
    "\n",
    "\n",
    "What might we use these representations for ? \n",
    "- Better understanding the structure of this dataset, how it is organized; how documents compare between themselves. With textual data, this is of course valuable in itself for applied linguistics and social sciences; we can basically look at how words are being used. \n",
    "- Serve as feature representations for other models: this is a first transformation to then feed data into another model to perform a task. \n",
    "\n",
    "What kind of representations will we use ?\n",
    "- Let us begin with something very simple. We will represent documents through **counts of word occurrences**. A very convenient way of doing so is to use a **Bag-of-Words (BoW)** vector, containing the counts of each word (regardless of their order of occurrence) in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"NLP_labs/PreProcessing_textual_features/gathered_tweets_labeled.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd68b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c65fa3",
   "metadata": {},
   "source": [
    "Instead of working with tables, we will put the data into *lists*; this will make it easier and more convenient to manipulate it at first. Try to understand exactly how we build the following lists, and why ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_l = [t for t in tweets['Tweet'].tolist() if len(t) > 0]\n",
    "categories_l = [c for c, t in zip(tweets['Category'].tolist(),tweets['Tweet'].tolist()) if len(t) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ca8ea",
   "metadata": {},
   "source": [
    "### II-1 Pre-processing: Tokenization and cleaning\n",
    "\n",
    "The first thing to do is then to turn each tweet from a string into a list of words. The simplest method is to divide the string according to spaces with the command:\n",
    "``text.split()``\n",
    "\n",
    "But we must also be careful to remove special characters that may not have been cleaned up (such as HTML tags if the data was obtained from web pages). Since we're going to count words, we'll have to build a **list of tokens** appearing in our data. In our case, we'd like to reduce this list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
    "\n",
    "\n",
    "We will therefore use a function adapted to our needs - but this is a job that we generally don't need to do ourselves, since there are many tools already adapted to most situations. \n",
    "For text cleansing, there are many scripts, based on different tools (regular expressions, for example) that allow you to prepare data. The division of the text into words and the management of punctuation is handled in a step called **tokenization**; if needed, a python package like NLTK contains many different *tokenizers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b33b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove words beginning by @ ? (Good choice ?)\n",
    "    # text = re.sub(r'()@\\w+', r'\\1', text)\n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example\n",
    "print(clean_and_tokenize(tweets_l[24]))\n",
    "print(word_tokenize(tweets_l[24]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b413423",
   "metadata": {},
   "source": [
    "- lowercase - upercase\n",
    "- @ and name - @name\n",
    "- punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole dataset\n",
    "clean_tweets_l = [clean_and_tokenize(t) for t in tweets_l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30553b",
   "metadata": {},
   "source": [
    "### III-2 Obtaning representations: bag-of-words\n",
    "\n",
    "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (the **vocabulary**), we can create an index, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
    "\n",
    "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
    "\n",
    "The next function takes as input a list of documents (again, each in the form of a string) and returns:\n",
    "- A vocabulary that associates, to each word encountered, an index\n",
    "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
    "\n",
    "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad831d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    # If the vocabulary is not known, we need to build it\n",
    "    if voc == None:\n",
    "        words =set()\n",
    "        for t in texts:\n",
    "            words = words.union(set(clean_and_tokenize(t)))\n",
    "        n_features = len(words)\n",
    "        \n",
    "        vocabulary = dict(zip(words, range(n_features)))\n",
    "        \n",
    "    # If it's given, it's quite easier\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "        n_features = len(voc)\n",
    "    \n",
    "    # Creating the matrix counts\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Filling the matrix by iterating over the documents and counting the words\n",
    "    for k, t in enumerate(texts): \n",
    "        for w in clean_and_tokenize(t):\n",
    "            counts[k][vocabulary[w]] += 1.\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, bow = count_words(tweets_l)\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2945d",
   "metadata": {},
   "source": [
    "We can also use ```scikit-learn```'s tools for this: the **CountVectorizer** class allows us to obtain the same kind of representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer()\n",
    "Bow = vectorizer.fit_transform(tweets_l)\n",
    "bow_a = Bow.toarray()\n",
    "print(bow_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6ac0f",
   "metadata": {},
   "source": [
    "Let's first look at the most frequent words. This will require some simple array manipulation:\n",
    "- Retrieving the sum of all word occurences across documents,\n",
    "- Sorting words according to their frequency,\n",
    "- Plotting an histogram for the top words, using the count as value and the word as legend.\n",
    "\n",
    "How can that influence our pre-processing ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = bow.sum(axis = 0)\n",
    "top_words = np.argsort(frequency)[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53791d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_voc = {i: w for w, i in voc.items()}\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(15), frequency[top_words[:15]])\n",
    "ax.set_xticks(range(15))\n",
    "ax.set_xticklabels([rev_voc[i] for i in top_words[:15]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6a1a7",
   "metadata": {},
   "source": [
    "### II-3 Vector comparison\n",
    "\n",
    "We can use these very large-dimensional vectors for a very simple semantic analysis: for example, by looking for the nearest neighbors of a tweet. \n",
    "However, we need to be careful to the distance that we use: should it be *Euclidean* or *Cosine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ec427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b1a3d",
   "metadata": {},
   "source": [
    "Implement a function using the ```NearestNeighbors``` class for ```sklearn```, allowing you to print the closest document of a reference index. Try both distances and both representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neighbors(distance, texts, representations, index, k=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        distance : function\n",
    "            The distance to use to compare documents\n",
    "        texts : list of str\n",
    "            The texts\n",
    "        representations: 2D Array\n",
    "            Vector representations of the texts, in the same order\n",
    "        index: int\n",
    "            Index of the document for which to return nearest neighbors\n",
    "        k: int\n",
    "            Number of neighbors to display  \n",
    "    Return: \n",
    "        display the nearest text neighbors   \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize NearestNeighbors\n",
    "    neighbors = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=distance)\n",
    "    neighbors.fit(representations)\n",
    "\n",
    "    # Get the indices of the nearest neighbors\n",
    "    distances, indices = neighbors.kneighbors([representations[index]])\n",
    "\n",
    "    print(f\"Nearest neighbors of:'{texts[index]}' \\n using distance '{distance.__name__}':\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx != index:  # Skip the reference document itself\n",
    "            print(f\"{i+1}. Distance: {dist:.4f} - Text: '{texts[idx]}'\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, tweets_l, bow, 24)\n",
    "print_neighbors(cosine, tweets_l, bow, 24)\n",
    "\n",
    "print_neighbors(euclidean, tweets_l, bow_a, 24)\n",
    "print_neighbors(cosine, tweets_l, bow_a, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7d18b",
   "metadata": {},
   "source": [
    "### II-4 Improving representations with TF-IDF\n",
    "\n",
    "This is the product of the frequency of the term (TF) and its inverse frequency in documents (IDF).\n",
    "This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. \n",
    "\n",
    "Implement a function transforming the BOW representations we obtained as output of ```count_words``` into TF-IDF representations. Do not forget about **smoothing** ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tfidf_transform(bow):\n",
    "    \"\"\"\n",
    "    Transform a bag-of-words matrix into a TF-IDF matrix.\n",
    "\n",
    "    Parameters:\n",
    "    bow (numpy.ndarray): A 2D array of shape (n_documents, n_terms).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The TF-IDF matrix\n",
    "    \"\"\"\n",
    "    # nb doc\n",
    "    D = bow.shape[0]\n",
    "    \n",
    "    # IDF\n",
    "    in_doc = (bow>0).sum(axis=0) # booleen\n",
    "    idf = np.log(D/in_doc+1) + 1 # smoothing and avoid /0\n",
    "\n",
    "    # TF\n",
    "    sum_vec = bow.sum(axis=1, keepdims=True) +1 # sum per line\n",
    "    tf = bow/sum_vec\n",
    "    tf_idf = normalize(idf*tf, norm='l2', axis=1)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform(bow)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "Tfidf = tfidf_vectorizer.fit_transform(tweets_l)\n",
    "tfidf_a = Tfidf.toarray()\n",
    "print(tfidf_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa855b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, tweets_l, tfidf_a, 24)\n",
    "print_neighbors(cosine, tweets_l, tfidf_a, 24)\n",
    "\n",
    "print_neighbors(euclidean, tweets_l, tfidf, 24)\n",
    "print_neighbors(cosine, tweets_l, tfidf, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9681f1",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Try to interpret the nearest neighbors for each distance - think about how they are computed. Which kind of documents are privileged, for each of them ? How might the representation affect this ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38f90a",
   "metadata": {},
   "source": [
    "## III - Data visualization\n",
    "\n",
    "**Question:** can these representations capture what the dataset is about ? We will use visualization to find out in this last section. More precisely, we would like to check if lexical features (which we tried to rid of the influence of word frequency with TF-IDF), are enough to capture the characteristics of the dataset (which are reflected in the **annotations**.\n",
    "\n",
    "We will not use supervised Machine Learning yet, but rather try to visualize our data in 2D: it's all about reducing the dimension with techniques that keep what's (statistically) more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784b863",
   "metadata": {},
   "source": [
    "### III - 1 With PCA\n",
    "\n",
    "We will now use **principal components analysis** (PCA) to visualize our data in two dimensions. This is equivalent to applying SVD to the covariance matrix of the data, in order for the principal components to be independant from each other an maximize the variance of the data. We use the class ```PCA``` from ```scikit-learn```.\n",
    "                        \n",
    "Working on TF-IDF representations and displaying classes: \n",
    "- What can we observe ?\n",
    "- How can we interpret this with respect to our features ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "docs_pca = pca.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05441ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_pca[:,0],\n",
    "                     'y': docs_pca[:,1],\n",
    "                     'tweet': tweets_l,\n",
    "                     'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['tweet']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea44c3",
   "metadata": {},
   "source": [
    "### III - 2 With T-SNE\n",
    "\n",
    "From the ```sklearn``` documentation: \n",
    "- t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. **with different initializations we can get different results**.\n",
    "- In particular, t-SNE has the advantage to reveal data that lie in multiple, different, manifolds or clusters.\n",
    "- It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.\n",
    "\n",
    "From this recommendation, we will initialize ```TSNE``` with PCA (choosing the argument ```init='pca'``` when creating the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f51222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca').fit_transform(tfidf)\n",
    "print(docs_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab325aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_tsne[:,0],\n",
    "                     'y': docs_tsne[:,1],\n",
    "                     'tweet': tweets_l,\n",
    "                     'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['tweet']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f9d88",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "                        \n",
    "- Is there any conclusion we can draw with respect to the lexical features and how they allow us to group the documents in this dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c7f96",
   "metadata": {},
   "source": [
    "### III - 3 Topic modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension before visualization. \n",
    "\n",
    "The underlying idea is to **take advantage of the latent structure in the association between the set of\n",
    "words and the set of documents**. Many methods have been designed to do this - the earliest being **topic models**. \n",
    "\n",
    "Note that this allows to obtain reduced document representations, in a **topic space, common to documents and words** - where each document is described as a vector of topics and for each topic, we have access to the importance of words. \n",
    "\n",
    "\n",
    "We will do this with two models:\n",
    "- Using the ```TruncatedSVD```, we will **linearly** reduce the dimension of our BOW representations. This is called *Latent Semantic Analysis* (LSA). \n",
    "- Using a *generative model* based on several assumptions on how a document is generated through topics, which the model will retrieve: this is ```LatentDirichletAllocation``` (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b165899",
   "metadata": {},
   "source": [
    "We use here another dataset from this [paper](https://aclanthology.org/2024.latechclfl-1.28/) which includes quite more categories and will be more interesting to explore, as we can expect it to contain clusters clearly visible through looking at lexical features. You can find the dataset on their [git repository](https://git.unistra.fr/thealtres/stage-direction-classif-french-transfer-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cec8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AS13_df = pd.read_table(\"NLP_labs/PreProcessing_textual_features/stgdir_labelGeneric.csv\",\n",
    "                        sep='|', \n",
    "                        dtype={'description' : 'object', 'labelGeneric': 'category', })\n",
    "labelCol = 'labelGeneric'\n",
    "class_names = sorted(AS13_df[labelCol].unique().categories.to_list())\n",
    "label2id = {class_names[i]:i for i in range(len(class_names))}\n",
    "id2label = {i:class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "AS13_df = AS13_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad1373",
   "metadata": {},
   "source": [
    "First, apply the same pipeline than before:\n",
    "- Does the data need to be cleaned and pre-processed ?\n",
    "- Obtain BOW and TF-IDF representations.\n",
    "- Visualize them with T-SNE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain BOW representations\n",
    "voc_th, bow_th = count_words(AS13_df['description'])\n",
    "print(bow_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform them into TF-IDF\n",
    "tfidf_th = tfidf_transform(bow_th)\n",
    "print(tfidf_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8582599",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='pca').fit_transform(tfidf_th)\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': AS13_df['description'],\n",
    "                        'Category': AS13_df['labelGeneric']})\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230bad2",
   "metadata": {},
   "source": [
    "**Latent Semantic Analysis**: let us choose an arbitrary number of topics - which will be the size of the joint *topic space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac15c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "lsa = TruncatedSVD(n_components = n_topics)\n",
    "lsa_topics = lsa.fit_transform(tfidf_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondances between documents and topics\n",
    "print(lsa_topics.shape)\n",
    "# Correspondances between topics and words\n",
    "print(lsa.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the vocabulary to retrieve words from indexes, allowing to find the most important words for each topic\n",
    "rev_voc_th = {i: w for w, i in voc_th.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_words(n, reverse_vocabulary, topic_model):\n",
    "    out = []\n",
    "    for i, topic in enumerate(topic_model.components_):\n",
    "        out.append([reverse_vocabulary[j] for j in topic.argsort()[:-n-1:-1]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lsa)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ffd3",
   "metadata": {},
   "source": [
    "With a dataset this size, over **short texts**, it is difficult to interpret the topics (many short words, even with TF-IDF). Let's apply T-SNE ! \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78923ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result of dimension reduction via T-SNE; display the classes\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f952cf5",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics)\n",
    "lda_topics_th = lda.fit_transform(bow_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lda)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f97120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_topics_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca', metric='cosine', perplexity=50.0).fit_transform(lda_topics_th)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': AS13_df['description'],\n",
    "                        'Category': AS13_df['labelGeneric']})\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964469f9",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "- Apply the pipeline to obtain a t-sne visualisation over the same representations, but for the **second dataset** (*gathered_tweets_labeled*). Did it work as expected ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462ba09",
   "metadata": {},
   "source": [
    "### III - 4 Take away\n",
    "\n",
    "**Idea**: the key to improving representations is to embed data capturing text statistics in a compact space.\n",
    "\n",
    "But how ? \n",
    "Let's look at how a compact **modern (deep learning based) model** can better capture what's happening in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
    "model = AutoModel.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is very inefficient: it will take documents one by one and make them go through the model\n",
    "# We can usually process several of them together to gain time: this is called batching\n",
    "# Batching may require a large quantity of memory, and to avoid any issue when running this locally,\n",
    "# we will keep this (very slow and) inefficient solution. \n",
    "vectors = []\n",
    "for i, example in enumerate(AS13_df['description'].tolist()):\n",
    "    inputs = tokenizer(example, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    vectors.append(outputs.last_hidden_state[0,0,:].detach().numpy()[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs vectors of size 768\n",
    "cam_rep = np.concatenate(vectors, axis=0)\n",
    "print(cam_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184611",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='random', metric='cosine',\n",
    "                    perplexity=50.0, square_distances=True).fit_transform(cam_rep)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': AS13_df['description'],\n",
    "                        'Category': AS13_df['labelGeneric']})\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd85c",
   "metadata": {},
   "source": [
    "We will see how such a model (*CamemBERT*) works in a few months ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
